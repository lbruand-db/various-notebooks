{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1006c0e3-d184-4502-8f62-bd1a35c85bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77dc818d-211f-4e6c-9b0f-e0a05be8363b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://data.geopf.fr/wmts?layer=ORTHOIMAGERY.ORTHOPHOTOS&style=normal&tilematrixset=PM&Service=WMTS&Request=GetTile&Version=1.0.0&Format=image%2Fjpeg&TileMatrix=18&TileCol=133305&TileRow=88135\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8098e01d-04ca-4149-b0e3-2c4abd659d81",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Image saving function"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def save_image(image_data, filename=None, directory=\".\", format=\"jpg\", add_timestamp=True):\n",
    "    \"\"\"\n",
    "    Save image data to disk with optional timestamp and custom directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_data : bytes\n",
    "        Raw image data to save\n",
    "    filename : str, optional\n",
    "        Base filename (without extension). If None, uses 'image'\n",
    "    directory : str, optional\n",
    "        Directory to save the image. Default is current directory\n",
    "    format : str, optional\n",
    "        Image format (jpg, png, etc.). Default is 'jpg'\n",
    "    add_timestamp : bool, optional\n",
    "        Whether to add timestamp to filename. Default is True\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Full path to the saved image\n",
    "    \"\"\"\n",
    "    # Set default filename if not provided\n",
    "    if filename is None:\n",
    "        filename = \"image\"\n",
    "    \n",
    "    # Remove extension if provided\n",
    "    filename = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Add timestamp if requested\n",
    "    if add_timestamp:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{filename}_{timestamp}\"\n",
    "    \n",
    "    # Add extension\n",
    "    filename = f\"{filename}.{format}\"\n",
    "    \n",
    "    # Create full path\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Check if file exists and create unique name if needed\n",
    "    if os.path.exists(filepath):\n",
    "        import time\n",
    "        base, ext = os.path.splitext(filepath)\n",
    "        filepath = f\"{base}_{int(time.time())}{ext}\"\n",
    "    \n",
    "    # Save the image\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(image_data)\n",
    "    \n",
    "    # Get file info\n",
    "    file_size = len(image_data)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "        width, height = img.size\n",
    "        img_format = img.format\n",
    "        print(f\"✓ Image saved successfully!\")\n",
    "        print(f\"  Path: {os.path.abspath(filepath)}\")\n",
    "        print(f\"  Size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")\n",
    "        print(f\"  Dimensions: {width} x {height} pixels\")\n",
    "        print(f\"  Format: {img_format}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✓ Image saved: {os.path.abspath(filepath)}\")\n",
    "        print(f\"  Size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")\n",
    "        print(f\"  (Could not read image metadata: {e})\")\n",
    "    \n",
    "    return os.path.abspath(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c5e890-e3f1-4849-b7b7-bf22d808da4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Couche = \"ORTHOIMAGERY.ORTHOPHOTOS\" # ORTHOIMAGERY.ORTHOPHOTOS\n",
    "#Couche = \"ELEVATION.ELEVATIONGRIDCOVERAGE.HIGHRES\"\n",
    "Style = \"normal\"\n",
    "format = \"image%2Fjpeg\"\n",
    "TileMatrixSet = \"PM\"\n",
    "TileMatrix = \"18\"\n",
    "TileRow = \"90173\" #\"88135\"\n",
    "TileCol = \"132782\" #\"133305\"\n",
    "#TileRow = \"88135\"\n",
    "#TileCol = \"133305\"\n",
    "\n",
    "url = f\"https://data.geopf.fr/wmts?Service=WMTS&Request=GetTile&Version=1.0.0&layer={Couche}&style={Style}&Format={format}&tilematrixset={TileMatrixSet}&TILEMATRIX={TileMatrix}&TILEROW={TileRow}&TILECOL={TileCol}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0498e498-f273-4fa0-a25a-40fbca967647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b1fff5-52e3-4722-8cfb-5ea027ea22fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import base64\n",
    "\n",
    "# Download the image from the URL\n",
    "#url = 'https://example.com/path/to/your/image.jpg'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "\n",
    "    image_data = response.read()\n",
    "\n",
    "# Convert image to base64\n",
    "image_base64 = base64.b64encode(image_data).decode('utf-8')\n",
    "\n",
    "# Display as HTML img tag\n",
    "html = f'<img src=\"data:image/jpeg;base64,{image_base64}\" alt=\"Map Tile\" />'\n",
    "displayHTML(html)\n",
    "\n",
    "# Save the RGB image\n",
    "rgb_saved_path = save_image(image_data, filename=\"rgb_orthoimagery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48621699-c5c6-404d-bb01-8296ebba666f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Size in bytes\n",
    "size_bytes = len(image_data)\n",
    "size_kb = size_bytes / 1024\n",
    "size_mb = size_kb / 1024\n",
    "\n",
    "print(f\"Image size: {size_bytes:,} bytes ({size_kb:.2f} KB, {size_mb:.2f} MB)\")\n",
    "\n",
    "# Get image dimensions (width x height)\n",
    "image = Image.open(io.BytesIO(image_data))\n",
    "width, height = image.size\n",
    "print(f\"Image dimensions: {width} x {height} pixels\")\n",
    "print(f\"Image format: {image.format}\")\n",
    "print(f\"Image mode: {image.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68bd6af7-3b2c-4b86-8a58-1df394b41dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Couche = \"IGNF_LIDAR-HD_MNS_ELEVATION.ELEVATIONGRIDCOVERAGE.SHADOW\"\n",
    "#\"ELEVATION.ELEVATIONGRIDCOVERAGE.HIGHRES.MNS.SHADOW\"\n",
    "Style = \"normal\"\n",
    "format = \"image%2Fpng\"\n",
    "TileMatrixSet = \"PM_0_18\"\n",
    "TileMatrix = \"18\"\n",
    "TileRow = \"90173\" #\"88135\"\n",
    "TileCol = \"132782\" #\"133305\"\n",
    "\n",
    "url = f\"https://data.geopf.fr/wmts?gp-ol-ext=1.0.0-beta.6-461&Service=WMTS&Request=GetTile&Version=1.0.0&layer={Couche}&style={Style}&Format={format}&tilematrixset={TileMatrixSet}&TILEMATRIX={TileMatrix}&TILEROW={TileRow}&TILECOL={TileCol}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb41702a-9dd1-4d58-b7a2-86aa7b6d5703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203f4c8b-630f-4f04-8c92-b324484d74fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import base64\n",
    "\n",
    "# Download the image from the URL\n",
    "#url = 'https://example.com/path/to/your/image.jpg'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "\n",
    "    image_data = response.read()\n",
    "\n",
    "# Convert image to base64\n",
    "image_base64 = base64.b64encode(image_data).decode('utf-8')\n",
    "\n",
    "# Display as HTML img tag\n",
    "html = f'<img src=\"data:image/jpeg;base64,{image_base64}\" alt=\"Map Tile\" />'\n",
    "displayHTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0928d261-858d-47c9-8588-443ef9b1cf6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Size in bytes\n",
    "size_bytes = len(image_data)\n",
    "size_kb = size_bytes / 1024\n",
    "size_mb = size_kb / 1024\n",
    "\n",
    "print(f\"Image size: {size_bytes:,} bytes ({size_kb:.2f} KB, {size_mb:.2f} MB)\")\n",
    "\n",
    "# Get image dimensions (width x height)\n",
    "image = Image.open(io.BytesIO(image_data))\n",
    "width, height = image.size\n",
    "print(f\"Image dimensions: {width} x {height} pixels\")\n",
    "print(f\"Image format: {image.format}\")\n",
    "print(f\"Image mode: {image.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409f2da6-fd03-42da-85ae-c356389261f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Save image using the function"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Save with default settings (adds timestamp)\n",
    "lidarhd_saved_path = save_image(image_data, filename=\"mns_lidarhd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa957b04-29c3-45bb-b5bd-1a855050b364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lidarhd_saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cede9e98-bd30-43d6-bf8f-414e830ce5b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import PyTorch libraries"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02410e3e-8866-4f98-ad35-a470170b3c5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define multi-modal ResNet50 model"
    }
   },
   "outputs": [],
   "source": [
    "class MultiModalResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet50 model that accepts both RGB (3 channels) and depth (1 channel) images.\n",
    "    Total input: 4 channels (RGB + Depth)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000, pretrained=True):\n",
    "        super(MultiModalResNet50, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        if pretrained:\n",
    "            weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "            self.resnet = models.resnet50(weights=weights)\n",
    "        else:\n",
    "            self.resnet = models.resnet50(weights=None)\n",
    "        \n",
    "        # Get the original first convolutional layer (3 input channels)\n",
    "        original_conv1 = self.resnet.conv1\n",
    "        \n",
    "        # Create new conv1 layer with 4 input channels (RGB + Depth)\n",
    "        self.resnet.conv1 = nn.Conv2d(\n",
    "            in_channels=4,  # RGB (3) + Depth (1)\n",
    "            out_channels=original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Initialize the new conv1 layer weights\n",
    "        with torch.no_grad():\n",
    "            # Copy pretrained weights for RGB channels\n",
    "            self.resnet.conv1.weight[:, :3, :, :] = original_conv1.weight\n",
    "            \n",
    "            # Initialize depth channel weights (4th channel)\n",
    "            # Option 1: Average of RGB weights (good for grayscale-like depth)\n",
    "            self.resnet.conv1.weight[:, 3:4, :, :] = original_conv1.weight.mean(dim=1, keepdim=True)\n",
    "            \n",
    "            # Option 2 (alternative): Initialize with small random values\n",
    "            # self.resnet.conv1.weight[:, 3:4, :, :] = torch.randn_like(original_conv1.weight[:, 0:1, :, :]) * 0.01\n",
    "        \n",
    "        # Modify the final fully connected layer if needed\n",
    "        if num_classes != 1000:\n",
    "            num_features = self.resnet.fc.in_features\n",
    "            self.resnet.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Forward pass with separate RGB and depth inputs.\n",
    "        \n",
    "        Args:\n",
    "            rgb: Tensor of shape (batch_size, 3, height, width) - RGB image\n",
    "            depth: Tensor of shape (batch_size, 1, height, width) - Depth image\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Concatenate RGB and depth along channel dimension\n",
    "        x = torch.cat([rgb, depth], dim=1)  # Shape: (batch_size, 4, height, width)\n",
    "        \n",
    "        # Pass through ResNet50\n",
    "        return self.resnet(x)\n",
    "    \n",
    "    def forward_single_input(self, rgbd):\n",
    "        \"\"\"\n",
    "        Alternative forward pass with pre-concatenated RGBD input.\n",
    "        \n",
    "        Args:\n",
    "            rgbd: Tensor of shape (batch_size, 4, height, width) - RGBD image\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        return self.resnet(rgbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72dcca17-1334-4b2c-9d7c-c0549026cda7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Instantiate and test the model"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model for binary classification (1 output)\n",
    "model = MultiModalResNet50(num_classes=1, pretrained=True)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model created and moved to: {device}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  - Input: RGB (3 channels) + Depth (1 channel) = 4 channels\")\n",
    "print(f\"  - Backbone: ResNet50 (pretrained on ImageNet)\")\n",
    "print(f\"  - Output: {model.resnet.fc.out_features} class (binary classification)\")\n",
    "\n",
    "# Test with dummy data\n",
    "batch_size = 2\n",
    "height, width = 224, 224  # Standard ResNet input size\n",
    "\n",
    "# Create dummy RGB and depth images\n",
    "rgb_dummy = torch.randn(batch_size, 3, height, width).to(device)\n",
    "depth_dummy = torch.randn(batch_size, 1, height, width).to(device)\n",
    "\n",
    "print(f\"\\nTesting with dummy data:\")\n",
    "print(f\"  RGB shape: {rgb_dummy.shape}\")\n",
    "print(f\"  Depth shape: {depth_dummy.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(rgb_dummy, depth_dummy)\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Raw output (logits): {output.squeeze().cpu().numpy()}\")\n",
    "    \n",
    "    # Apply sigmoid for probability\n",
    "    probabilities = torch.sigmoid(output)\n",
    "    print(f\"  Probabilities: {probabilities.squeeze().cpu().numpy()}\")\n",
    "    print(f\"\\n✓ Model is ready for binary classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08af0263-57fc-408d-96fa-24052458550e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper: Load and preprocess your images"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_images(rgb_image_data, depth_image_data, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess RGB and depth images for the model.\n",
    "    \n",
    "    Args:\n",
    "        rgb_image_data: PIL Image or numpy array (H, W, 3)\n",
    "        depth_image_data: PIL Image or numpy array (H, W) or (H, W, 1)\n",
    "        target_size: Tuple (height, width) for resizing\n",
    "    \n",
    "    Returns:\n",
    "        rgb_tensor: Tensor of shape (1, 3, H, W)\n",
    "        depth_tensor: Tensor of shape (1, 1, H, W)\n",
    "    \"\"\"\n",
    "    # Define transforms for RGB (ImageNet normalization)\n",
    "    rgb_transform = transforms.Compose([\n",
    "        transforms.ToPILImage() if isinstance(rgb_image_data, np.ndarray) else transforms.Lambda(lambda x: x),\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Define transforms for depth (normalize to [0, 1])\n",
    "    depth_transform = transforms.Compose([\n",
    "        transforms.ToPILImage() if isinstance(depth_image_data, np.ndarray) else transforms.Lambda(lambda x: x),\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        # Optional: normalize depth values\n",
    "        # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    # Apply transforms\n",
    "    rgb_tensor = rgb_transform(rgb_image_data).unsqueeze(0)  # Add batch dimension\n",
    "    depth_tensor = depth_transform(depth_image_data).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    return rgb_tensor, depth_tensor\n",
    "\n",
    "print(\"Preprocessing helper function defined.\")\n",
    "print(\"Use: rgb_tensor, depth_tensor = preprocess_images(rgb_img, depth_img)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ea81e0-4f96-42cb-a098-4185a09e2859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load RGB image data from saved file\n",
    "with open(rgb_saved_path, 'rb') as f:\n",
    "    image_data_rgb = f.read()\n",
    "\n",
    "print(f\"Loaded RGB image from: {rgb_saved_path}\")\n",
    "print(f\"Image data size: {len(image_data_rgb):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cc8987-5f8f-42b6-a048-ea09ab7a7529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load LiDAR HD depth image data from saved file\n",
    "with open(lidarhd_saved_path, 'rb') as f:\n",
    "    image_data_depth = f.read()\n",
    "\n",
    "print(f\"Loaded LiDAR HD depth image from: {lidarhd_saved_path}\")\n",
    "print(f\"Image data size: {len(image_data_depth):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32667831-3db3-4895-a528-6b43b946b85f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Use with your actual images"
    }
   },
   "outputs": [],
   "source": [
    "# Example of how to use the model with your loaded images for binary classification\n",
    "# Assuming you have 'image_data' from cells 5 and 9\n",
    "\n",
    "# You would need to:\n",
    "# 1. Load both RGB and depth images\n",
    "# 2. Convert them to PIL Images or numpy arrays\n",
    "# 3. Preprocess them\n",
    "# 4. Run inference with sigmoid for binary classification\n",
    "\n",
    "# Example workflow (uncomment and adapt to your data):\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Load RGB image (from cell 5)\n",
    "rgb_image = Image.open(io.BytesIO(image_data_rgb))  # Your RGB orthoimagery\n",
    "\n",
    "# Load depth image (from cell 9)\n",
    "depth_image = Image.open(io.BytesIO(image_data_depth))  # Your LiDAR HD depth\n",
    "depth_image = depth_image.convert('L')  # Convert to grayscale if needed\n",
    "\n",
    "# Preprocess\n",
    "rgb_tensor, depth_tensor = preprocess_images(rgb_image, depth_image)\n",
    "\n",
    "# Move to device\n",
    "rgb_tensor = rgb_tensor.to(device)\n",
    "depth_tensor = depth_tensor.to(device)\n",
    "\n",
    "# Run inference for binary classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(rgb_tensor, depth_tensor)\n",
    "    probability = torch.sigmoid(logits).item()\n",
    "    prediction = 1 if probability > 0.5 else 0\n",
    "    \n",
    "    print(f\"Binary Classification Results:\")\n",
    "    print(f\"  Probability: {probability:.4f}\")\n",
    "    print(f\"  Prediction: {prediction} ({'Positive' if prediction == 1 else 'Negative'})\")\n",
    "\n",
    "\n",
    "print(\"Example workflow for binary classification provided above (commented out).\")\n",
    "print(\"Adapt it to use your actual RGB and depth image data.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "orthoimagery_resnet50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
